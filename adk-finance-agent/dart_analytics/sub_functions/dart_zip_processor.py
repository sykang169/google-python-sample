"""
DART ZIP íŒŒì¼ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
ê³µì‹œì„œë¥˜ ì›ë³¸íŒŒì¼(ZIP)ì„ ìë™ìœ¼ë¡œ ë¶„ì„í•˜ê³  êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""

import zipfile
import os
import tempfile
import shutil
from pathlib import Path
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import re
import json
from typing import Dict, List, Optional, Any
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DartZipProcessor:
    """DART ZIP íŒŒì¼ ì²˜ë¦¬ ë° ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.temp_dirs = []
        self.supported_extensions = {'.xml', '.html', '.htm', '.txt', '.pdf', '.hwp', '.doc', '.docx'}
        
    def __del__(self):
        """ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬"""
        self.cleanup_temp_dirs()
    
    def cleanup_temp_dirs(self):
        """ìƒì„±ëœ ì„ì‹œ ë””ë ‰í† ë¦¬ë“¤ì„ ì •ë¦¬"""
        for temp_dir in self.temp_dirs:
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir, ignore_errors=True)
        self.temp_dirs.clear()

    def analyze_dart_zip_file(self, zip_file_path: str) -> Dict[str, Any]:
        """
        DART ZIP íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì••ì¶• í•´ì œí•˜ê³  ë‚´ìš©ì„ ë¶„ì„
        
        Args:
            zip_file_path: ZIP íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        analysis_result = {
            "status": "success",
            "files": [],
            "main_document": None,
            "attachments": [],
            "financial_files": [],
            "summary": "",
            "file_count": 0,
            "total_size": 0
        }
        
        try:
            if not os.path.exists(zip_file_path):
                analysis_result["status"] = "error"
                analysis_result["error"] = "ZIP íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                return analysis_result
            
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            temp_dir = tempfile.mkdtemp(prefix="dart_analysis_")
            self.temp_dirs.append(temp_dir)
            
            # ZIP íŒŒì¼ ì••ì¶• í•´ì œ (ì¸ì½”ë”© ë¬¸ì œ ìš°íšŒ)
            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
                zip_ref.extractall(temp_dir)
            
            # ì••ì¶• í•´ì œëœ íŒŒì¼ë“¤ ë¶„ì„
            for root, dirs, files in os.walk(temp_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    try:
                        file_info = self._analyze_file(file_path)
                        analysis_result["files"].append(file_info)
                        analysis_result["file_count"] += 1
                        analysis_result["total_size"] += file_info["size"]
                        
                        # íŒŒì¼ ë¶„ë¥˜
                        self._classify_file(file_info, analysis_result)
                        
                    except Exception as e:
                        logger.warning(f"íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ {file}: {str(e)}")
            
            # ìš”ì•½ ìƒì„±
            analysis_result["summary"] = self._generate_file_summary(analysis_result)
            
        except Exception as e:
            analysis_result["status"] = "error"
            analysis_result["error"] = f"ZIP íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            logger.error(f"ZIP ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        
        return analysis_result

    def _analyze_file(self, file_path: str) -> Dict[str, Any]:
        """ê°œë³„ íŒŒì¼ ë¶„ì„"""
        file_info = {
            "name": os.path.basename(file_path),
            "path": file_path,
            "size": os.path.getsize(file_path),
            "type": Path(file_path).suffix.lower(),
            "content_summary": "",
            "encoding": "unknown",
            "is_main_document": False,
            "document_type": "unknown"
        }
        
        # íŒŒì¼ ìœ í˜•ë³„ ë¶„ì„
        if file_info["type"] in ['.xml', '.html', '.htm']:
            file_info["content_summary"] = self._extract_xml_html_summary(file_path)
            file_info["document_type"] = self._identify_document_type(file_info["name"])
            file_info["is_main_document"] = self._is_main_document(file_info["name"])
        elif file_info["type"] == '.txt':
            file_info["content_summary"] = self._extract_text_summary(file_path)
        
        return file_info

    def _classify_file(self, file_info: Dict[str, Any], analysis_result: Dict[str, Any]):
        """íŒŒì¼ì„ ìœ í˜•ë³„ë¡œ ë¶„ë¥˜"""
        if file_info["is_main_document"]:
            if analysis_result["main_document"] is None:
                analysis_result["main_document"] = file_info
        elif any(keyword in file_info["name"].lower() for keyword in 
                ['ì¬ë¬´ì œí‘œ', 'ì†ìµê³„ì‚°ì„œ', 'ì¬ë¬´ìƒíƒœí‘œ', 'í˜„ê¸ˆíë¦„í‘œ', 'ìë³¸ë³€ë™í‘œ']):
            analysis_result["financial_files"].append(file_info)
        elif file_info["type"] in ['.pdf', '.hwp', '.doc', '.docx']:
            analysis_result["attachments"].append(file_info)

    def _extract_xml_html_summary(self, file_path: str) -> str:
        """XML/HTML íŒŒì¼ì—ì„œ ì£¼ìš” ë‚´ìš© ì¶”ì¶œ"""
        try:
            # ì¸ì½”ë”© ì‹œë„ ëª©ë¡
            encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1']
            content = None
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read()
                    break
                except UnicodeDecodeError:
                    continue
            
            if content is None:
                return "ì¸ì½”ë”© ì˜¤ë¥˜ë¡œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(content, 'html.parser')
            
            # ì£¼ìš” ì •ë³´ ì¶”ì¶œ
            summary_parts = []
            
            # ì œëª© ì •ë³´
            title_tags = soup.find_all(['title', 'h1', 'h2'])
            for tag in title_tags[:3]:
                if tag.get_text().strip():
                    summary_parts.append(f"ì œëª©: {tag.get_text().strip()}")
            
            # í…Œì´ë¸” ì •ë³´ (ì¬ë¬´ì œí‘œ ë“±)
            tables = soup.find_all('table')
            if tables:
                summary_parts.append(f"í…Œì´ë¸” {len(tables)}ê°œ ë°œê²¬")
            
            # íšŒì‚¬ëª…, ë³´ê³ ì„œëª… ë“± í‚¤ì›Œë“œ ê²€ìƒ‰
            keywords = ['íšŒì‚¬ëª…', 'ë²•ì¸ëª…', 'ë³´ê³ ì„œ', 'ì‚¬ì—…ì—°ë„', 'ì ‘ìˆ˜ë²ˆí˜¸']
            for keyword in keywords:
                elements = soup.find_all(text=re.compile(keyword))
                for elem in elements[:2]:
                    parent_text = elem.parent.get_text().strip()
                    if len(parent_text) < 200:
                        summary_parts.append(parent_text)
            
            return "\n".join(summary_parts[:10])
            
        except Exception as e:
            return f"ë¶„ì„ ì˜¤ë¥˜: {str(e)}"

    def _extract_text_summary(self, file_path: str) -> str:
        """í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ìš”ì•½ ì¶”ì¶œ"""
        try:
            encodings = ['utf-8', 'cp949', 'euc-kr']
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read()
                    
                    # ì²˜ìŒ 500ìë§Œ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
                    summary = content[:500].strip()
                    if len(content) > 500:
                        summary += "..."
                    
                    return summary
                except UnicodeDecodeError:
                    continue
            
            return "í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì˜¤ë¥˜"
        except Exception as e:
            return f"í…ìŠ¤íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}"

    def _identify_document_type(self, filename: str) -> str:
        """íŒŒì¼ëª…ì„ í†µí•´ ë¬¸ì„œ ìœ í˜• ì‹ë³„"""
        filename_lower = filename.lower()
        
        if any(keyword in filename_lower for keyword in ['ì‚¬ì—…ë³´ê³ ì„œ', 'business']):
            return "ì‚¬ì—…ë³´ê³ ì„œ"
        elif any(keyword in filename_lower for keyword in ['ê°ì‚¬ë³´ê³ ì„œ', 'audit']):
            return "ê°ì‚¬ë³´ê³ ì„œ"
        elif any(keyword in filename_lower for keyword in ['ì¬ë¬´ì œí‘œ', 'financial']):
            return "ì¬ë¬´ì œí‘œ"
        elif any(keyword in filename_lower for keyword in ['ì²¨ë¶€', 'attachment']):
            return "ì²¨ë¶€íŒŒì¼"
        else:
            return "ê¸°íƒ€ë¬¸ì„œ"

    def _is_main_document(self, filename: str) -> bool:
        """ì£¼ìš” ë¬¸ì„œì¸ì§€ íŒë³„"""
        filename_lower = filename.lower()
        main_keywords = ['ì‚¬ì—…ë³´ê³ ì„œ', 'ê°ì‚¬ë³´ê³ ì„œ', 'business', 'audit']
        return any(keyword in filename_lower for keyword in main_keywords)

    def _generate_file_summary(self, analysis_result: Dict[str, Any]) -> str:
        """íŒŒì¼ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        summary_parts = []
        
        summary_parts.append(f"ì´ {analysis_result['file_count']}ê°œ íŒŒì¼ ë¶„ì„ ì™„ë£Œ")
        summary_parts.append(f"ì „ì²´ í¬ê¸°: {analysis_result['total_size'] / 1024 / 1024:.2f} MB")
        
        if analysis_result["main_document"]:
            summary_parts.append(f"ì£¼ìš” ë¬¸ì„œ: {analysis_result['main_document']['name']}")
        
        if analysis_result["financial_files"]:
            summary_parts.append(f"ì¬ë¬´ê´€ë ¨ íŒŒì¼: {len(analysis_result['financial_files'])}ê°œ")
        
        if analysis_result["attachments"]:
            summary_parts.append(f"ì²¨ë¶€íŒŒì¼: {len(analysis_result['attachments'])}ê°œ")
        
        return " | ".join(summary_parts)

    def extract_structured_data(self, analysis_result: Dict[str, Any], focus_area: str = "all") -> Dict[str, Any]:
        """
        ë¶„ì„ëœ íŒŒì¼ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
        
        Args:
            analysis_result: analyze_dart_zip_fileì˜ ê²°ê³¼
            focus_area: ë¶„ì„ ì´ˆì  ì˜ì—­ ("financial", "governance", "business", "all")
        """
        structured_data = {
            "focus_area": focus_area,
            "extracted_sections": {},
            "financial_data": {},
            "governance_data": {},
            "business_data": {},
            "metadata": {}
        }
        
        try:
            focus_keywords = self._get_focus_keywords(focus_area)
            
            # ì£¼ìš” ë¬¸ì„œì—ì„œ ë°ì´í„° ì¶”ì¶œ
            if analysis_result.get("main_document"):
                main_doc_data = self._extract_from_main_document(
                    analysis_result["main_document"], 
                    focus_keywords
                )
                structured_data["extracted_sections"]["main_document"] = main_doc_data
            
            # ì¬ë¬´ íŒŒì¼ì—ì„œ ë°ì´í„° ì¶”ì¶œ
            for financial_file in analysis_result.get("financial_files", []):
                financial_data = self._extract_financial_data(financial_file)
                if financial_data:
                    structured_data["financial_data"][financial_file["name"]] = financial_data
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            structured_data["metadata"] = {
                "extraction_time": self._get_current_time(),
                "file_count": analysis_result.get("file_count", 0),
                "focus_area": focus_area
            }
            
        except Exception as e:
            logger.error(f"êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            structured_data["error"] = str(e)
        
        return structured_data

    def _get_focus_keywords(self, focus_area: str) -> List[str]:
        """ì´ˆì  ì˜ì—­ë³„ í‚¤ì›Œë“œ ë°˜í™˜"""
        keywords_map = {
            "financial": ["ì¬ë¬´ìƒíƒœí‘œ", "ì†ìµê³„ì‚°ì„œ", "í˜„ê¸ˆíë¦„í‘œ", "ìë³¸ë³€ë™í‘œ", "ë§¤ì¶œ", "ìì‚°", "ë¶€ì±„", "ìë³¸"],
            "governance": ["ì„ì›í˜„í™©", "ì£¼ì£¼í˜„í™©", "ê°ì‚¬ì˜ê²¬", "ì§€ë°°êµ¬ì¡°", "ì´ì‚¬íšŒ", "ê°ì‚¬ìœ„ì›íšŒ"],
            "business": ["ì‚¬ì—…ê°œìš”", "ì£¼ìš”ì‚¬ì—…", "ì˜ì—…ì‹¤ì ", "ì‹œì¥ì ìœ ìœ¨", "ê²½ìŸí˜„í™©", "ì‚¬ì—…ì „ëµ"],
            "all": []
        }
        return keywords_map.get(focus_area, [])

    def _extract_from_main_document(self, main_doc: Dict[str, Any], focus_keywords: List[str]) -> Dict[str, Any]:
        """ì£¼ìš” ë¬¸ì„œì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        extracted_data = {
            "document_info": main_doc,
            "sections": {},
            "tables": [],
            "key_information": {}
        }
        
        try:
            if main_doc["type"] in ['.xml', '.html', '.htm']:
                with open(main_doc["path"], 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                soup = BeautifulSoup(content, 'html.parser')
                
                # í…Œì´ë¸” ì¶”ì¶œ
                tables = soup.find_all('table')
                for i, table in enumerate(tables[:5]):  # ìƒìœ„ 5ê°œ í…Œì´ë¸”ë§Œ
                    table_data = self._parse_table(table)
                    if table_data:
                        extracted_data["tables"].append({
                            "table_id": i,
                            "data": table_data
                        })
                
                # í‚¤ì›Œë“œ ê¸°ë°˜ ì„¹ì…˜ ì¶”ì¶œ
                if focus_keywords:
                    for keyword in focus_keywords:
                        sections = self._find_sections_by_keyword(soup, keyword)
                        if sections:
                            extracted_data["sections"][keyword] = sections
        
        except Exception as e:
            extracted_data["error"] = f"ë¬¸ì„œ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}"
        
        return extracted_data

    def _parse_table(self, table_element) -> List[List[str]]:
        """HTML í…Œì´ë¸”ì„ íŒŒì‹±í•˜ì—¬ 2ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        try:
            rows = table_element.find_all('tr')
            table_data = []
            
            for row in rows:
                cells = row.find_all(['td', 'th'])
                row_data = [cell.get_text().strip() for cell in cells]
                if any(cell for cell in row_data):  # ë¹ˆ í–‰ ì œì™¸
                    table_data.append(row_data)
            
            return table_data[:20]  # ìµœëŒ€ 20í–‰ê¹Œì§€ë§Œ
        except Exception:
            return []

    def _find_sections_by_keyword(self, soup, keyword: str) -> List[str]:
        """í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ì„¹ì…˜ ì°¾ê¸°"""
        sections = []
        
        # í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ìš”ì†Œ ì°¾ê¸°
        elements = soup.find_all(text=re.compile(keyword, re.IGNORECASE))
        
        for element in elements[:3]:  # ìƒìœ„ 3ê°œê¹Œì§€ë§Œ
            parent = element.parent
            if parent:
                section_text = parent.get_text().strip()
                if len(section_text) < 1000:  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ì œì™¸
                    sections.append(section_text)
        
        return sections

    def _extract_financial_data(self, financial_file: Dict[str, Any]) -> Dict[str, Any]:
        """ì¬ë¬´ íŒŒì¼ì—ì„œ ì¬ë¬´ ë°ì´í„° ì¶”ì¶œ"""
        financial_data = {
            "file_info": financial_file,
            "financial_statements": {},
            "key_figures": {}
        }
        
        try:
            if financial_file["type"] in ['.xml', '.html', '.htm']:
                with open(financial_file["path"], 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                soup = BeautifulSoup(content, 'html.parser')
                
                # ì¬ë¬´ì œí‘œ ê´€ë ¨ í…Œì´ë¸” ì°¾ê¸°
                tables = soup.find_all('table')
                for table in tables:
                    table_text = table.get_text().lower()
                    
                    if any(keyword in table_text for keyword in ['ìì‚°', 'ë¶€ì±„', 'ìë³¸']):
                        financial_data["financial_statements"]["balance_sheet"] = self._parse_table(table)
                    elif any(keyword in table_text for keyword in ['ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'ìˆœì´ìµ']):
                        financial_data["financial_statements"]["income_statement"] = self._parse_table(table)
                    elif any(keyword in table_text for keyword in ['í˜„ê¸ˆíë¦„', 'ì˜ì—…í™œë™']):
                        financial_data["financial_statements"]["cash_flow"] = self._parse_table(table)
        
        except Exception as e:
            financial_data["error"] = f"ì¬ë¬´ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}"
        
        return financial_data

    def _get_current_time(self) -> str:
        """í˜„ì¬ ì‹œê°„ ë°˜í™˜"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    def process_document_zip(self, zip_file_path: str, user_query: str = "", analysis_focus: str = "all") -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ì¶° ZIP íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ë‹µë³€ ìƒì„±
        
        Args:
            zip_file_path: ZIP íŒŒì¼ ê²½ë¡œ
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            analysis_focus: ë¶„ì„ ì´ˆì  ("financial", "governance", "business", "all")
        """
        logger.info(f"ZIP íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {zip_file_path}")
        
        # 1ë‹¨ê³„: ZIP íŒŒì¼ ë¶„ì„
        analysis_result = self.analyze_dart_zip_file(zip_file_path)
        
        if analysis_result["status"] != "success":
            return {
                "status": "error",
                "message": "ZIP íŒŒì¼ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "error": analysis_result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
            }
        
        # 2ë‹¨ê³„: ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ ë¶„ì„ ì´ˆì  ê²°ì •
        if analysis_focus == "all":
            analysis_focus = self._determine_focus_from_query(user_query)
        
        # 3ë‹¨ê³„: êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
        structured_data = self.extract_structured_data(analysis_result, analysis_focus)
        
        # 4ë‹¨ê³„: ì‘ë‹µ ìƒì„±
        response = self._generate_response(analysis_result, structured_data, user_query)
        
        return response

    def _determine_focus_from_query(self, user_query: str) -> str:
        """ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ë¶„ì„ ì´ˆì  ê²°ì •"""
        query_lower = user_query.lower()
        
        if any(keyword in query_lower for keyword in ['ì¬ë¬´', 'ë§¤ì¶œ', 'ìì‚°', 'ë¶€ì±„', 'ìˆœì´ìµ', 'ì˜ì—…ì´ìµ']):
            return "financial"
        elif any(keyword in query_lower for keyword in ['ì„ì›', 'ì£¼ì£¼', 'ì§€ë°°êµ¬ì¡°', 'ì´ì‚¬íšŒ', 'ê°ì‚¬']):
            return "governance"
        elif any(keyword in query_lower for keyword in ['ì‚¬ì—…', 'ì˜ì—…', 'ì‹œì¥', 'ê²½ìŸ', 'ì „ëµ']):
            return "business"
        else:
            return "all"

    def _generate_response(self, analysis_result: Dict[str, Any], structured_data: Dict[str, Any], user_query: str) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        response = {
            "status": "success",
            "query": user_query,
            "analysis_summary": analysis_result["summary"],
            "focus_area": structured_data["focus_area"],
            "key_findings": [],
            "detailed_data": {},
            "recommendations": []
        }
        
        # ì£¼ìš” ë°œê²¬ì‚¬í•­ ìƒì„±
        if analysis_result.get("main_document"):
            response["key_findings"].append(
                f"ì£¼ìš” ë¬¸ì„œ ë°œê²¬: {analysis_result['main_document']['name']}"
            )
        
        if structured_data.get("financial_data"):
            response["key_findings"].append(
                f"ì¬ë¬´ ë°ì´í„° {len(structured_data['financial_data'])}ê°œ íŒŒì¼ì—ì„œ ì¶”ì¶œ"
            )
        
        # ìƒì„¸ ë°ì´í„° í¬í•¨
        response["detailed_data"] = {
            "file_structure": analysis_result,
            "extracted_content": structured_data
        }
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        response["recommendations"] = self._generate_recommendations(structured_data)
        
        return response

    def _generate_recommendations(self, structured_data: Dict[str, Any]) -> List[str]:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if structured_data.get("financial_data"):
            recommendations.append("ğŸ’° ì¬ë¬´ì œí‘œ ë°ì´í„°ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì¬ë¬´ë¹„ìœ¨ ë¶„ì„ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        if "governance" in str(structured_data).lower():
            recommendations.append("ğŸ‘¥ ì§€ë°°êµ¬ì¡° ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì„ì› ë° ì£¼ì£¼ í˜„í™© ë¶„ì„ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        if structured_data.get("extracted_sections", {}).get("main_document", {}).get("tables"):
            recommendations.append("ğŸ“Š í‘œ í˜•íƒœì˜ êµ¬ì¡°í™”ëœ ë°ì´í„°ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì„¸ë¶€ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        if not recommendations:
            recommendations.append("ğŸ“‹ ê¸°ë³¸ì ì¸ ê³µì‹œì„œë¥˜ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return recommendations

    def format_for_display(self, response: Dict[str, Any]) -> str:
        """ì‚¬ìš©ìì—ê²Œ í‘œì‹œí•  í˜•íƒœë¡œ ì‘ë‹µ í¬ë§·íŒ…"""
        display_text = []
        
        display_text.append("ğŸ“ DART ê³µì‹œì„œë¥˜ ë¶„ì„ ê²°ê³¼")
        display_text.append("=" * 50)
        
        # ìš”ì•½ ì •ë³´
        display_text.append(f"ğŸ“‹ ë¶„ì„ ìš”ì•½: {response['analysis_summary']}")
        display_text.append(f"ğŸ¯ ë¶„ì„ ì´ˆì : {response['focus_area']}")
        
        # ì£¼ìš” ë°œê²¬ì‚¬í•­
        if response["key_findings"]:
            display_text.append("\nğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­:")
            for finding in response["key_findings"]:
                display_text.append(f"  â€¢ {finding}")
        
        # ê¶Œì¥ì‚¬í•­
        if response["recommendations"]:
            display_text.append("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
            for recommendation in response["recommendations"]:
                display_text.append(f"  â€¢ {recommendation}")
        
        # íŒŒì¼ êµ¬ì¡° ìš”ì•½
        file_structure = response["detailed_data"]["file_structure"]
        display_text.append(f"\nğŸ“‚ íŒŒì¼ êµ¬ì¡°:")
        display_text.append(f"  â€¢ ì´ íŒŒì¼ ìˆ˜: {file_structure['file_count']}ê°œ")
        display_text.append(f"  â€¢ ì „ì²´ í¬ê¸°: {file_structure['total_size'] / 1024 / 1024:.2f} MB")
        
        if file_structure.get("main_document"):
            display_text.append(f"  â€¢ ì£¼ìš” ë¬¸ì„œ: {file_structure['main_document']['name']}")
        
        if file_structure.get("financial_files"):
            display_text.append(f"  â€¢ ì¬ë¬´ ê´€ë ¨ íŒŒì¼: {len(file_structure['financial_files'])}ê°œ")
        
        if file_structure.get("attachments"):
            display_text.append(f"  â€¢ ì²¨ë¶€íŒŒì¼: {len(file_structure['attachments'])}ê°œ")
        
        return "\n".join(display_text)