"""
DART ë¬¸ì„œ ë¶„ì„ ëª¨ë“ˆ

ê³µì‹œì„œë¥˜ì˜ ë¶„ì„, XML íŒŒì‹±, ë‚´ìš© ì¶”ì¶œ ë“±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import os
import re
import zipfile
from pathlib import Path
from bs4 import BeautifulSoup
from .dart_zip_processor import DartZipProcessor
from .file_handlers import download_document_zip


# ZIP íŒŒì¼ ì²˜ë¦¬ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
zip_processor = DartZipProcessor()


def check_extracted_files_exist(rcept_no: str, download_folder: str = "./downloads") -> str:
    """
    ì´ë¯¸ ì••ì¶• í•´ì œëœ íŒŒì¼ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    
    Args:
        rcept_no: ì ‘ìˆ˜ë²ˆí˜¸ (14ìë¦¬)
        download_folder: ë‹¤ìš´ë¡œë“œ í´ë” ê²½ë¡œ
        
    Returns:
        íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ë° íŒŒì¼ ëª©ë¡
    """
    try:
        extract_folder = os.path.join(download_folder, f"extracted_{rcept_no}")
        
        if not os.path.exists(extract_folder):
            return f"âŒ ì••ì¶• í•´ì œëœ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {extract_folder}"
        
        # ì••ì¶• í•´ì œëœ íŒŒì¼ ëª©ë¡ í™•ì¸
        extracted_files = []
        for root, dirs, files in os.walk(extract_folder):
            for file in files:
                file_path = os.path.join(root, file)
                extracted_files.append({
                    'name': file,
                    'path': file_path,
                    'size_kb': os.path.getsize(file_path) / 1024
                })
        
        result = []
        result.append(f"âœ… ì••ì¶• í•´ì œëœ íŒŒì¼ë“¤ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")
        result.append(f"ğŸ“ í´ë” ê²½ë¡œ: {extract_folder}")
        result.append(f"ğŸ“‹ ì´ {len(extracted_files)}ê°œ íŒŒì¼")
        
        if extracted_files:
            result.append("\nğŸ“„ íŒŒì¼ ëª©ë¡:")
            for file_info in extracted_files[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                result.append(f"   â€¢ {file_info['name']} ({file_info['size_kb']:.1f} KB)")
            
            if len(extracted_files) > 10:
                result.append(f"   ... ë° {len(extracted_files) - 10}ê°œ ì¶”ê°€ íŒŒì¼")
        
        result.append("\nğŸ’¡ ë¶„ì„ì„ ì›í•˜ì‹œë©´ analyze_extracted_dart_document í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        
        return "\n".join(result)
        
    except Exception as e:
        return f"âŒ íŒŒì¼ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def read_extracted_file_content(rcept_no: str, filename: str, download_folder: str = "./downloads") -> str:
    """
    ì••ì¶• í•´ì œëœ íŠ¹ì • íŒŒì¼ì˜ ë‚´ìš©ì„ ì½ì–´ì„œ ë³´ì—¬ì¤ë‹ˆë‹¤.
    
    Args:
        rcept_no: ì ‘ìˆ˜ë²ˆí˜¸ (14ìë¦¬)
        filename: ì½ì„ íŒŒì¼ëª… (í™•ì¥ì í¬í•¨)
        download_folder: ë‹¤ìš´ë¡œë“œ í´ë” ê²½ë¡œ
        
    Returns:
        íŒŒì¼ ë‚´ìš© ë˜ëŠ” ì˜¤ë¥˜ ë©”ì‹œì§€
    """
    try:
        extract_folder = os.path.join(download_folder, f"extracted_{rcept_no}")
        
        if not os.path.exists(extract_folder):
            return f"âŒ ì••ì¶• í•´ì œëœ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {extract_folder}\në¨¼ì € download_and_extract_dart_document í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        
        # íŒŒì¼ ì°¾ê¸°
        target_file = None
        for root, dirs, files in os.walk(extract_folder):
            for file in files:
                if file == filename or file.lower() == filename.lower():
                    target_file = os.path.join(root, file)
                    break
            if target_file:
                break
        
        if not target_file:
            # ì‚¬ìš©ê°€ëŠ¥í•œ íŒŒì¼ ëª©ë¡ ì œê³µ
            available_files = []
            for root, dirs, files in os.walk(extract_folder):
                for file in files:
                    available_files.append(file)
            
            result = [f"âŒ '{filename}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
            result.append("\nğŸ“„ ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ë“¤:")
            for file in available_files[:10]:
                result.append(f"   â€¢ {file}")
            if len(available_files) > 10:
                result.append(f"   ... ë° {len(available_files) - 10}ê°œ ì¶”ê°€ íŒŒì¼")
            
            return "\n".join(result)
        
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        file_size = os.path.getsize(target_file)
        file_ext = os.path.splitext(target_file)[1].lower()
        
        result = []
        result.append(f"ğŸ“„ íŒŒì¼ ë‚´ìš©: {filename}")
        result.append("=" * 50)
        result.append(f"ğŸ“ íŒŒì¼ ê²½ë¡œ: {target_file}")
        result.append(f"ğŸ’¾ íŒŒì¼ í¬ê¸°: {file_size / 1024:.1f} KB")
        result.append("")
        
        # íŒŒì¼ ìœ í˜•ë³„ ì²˜ë¦¬
        if file_ext in ['.xml', '.html', '.htm']:
            # XML/HTML íŒŒì¼ ì²˜ë¦¬
            encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1']
            content = None
            
            for encoding in encodings:
                try:
                    with open(target_file, 'r', encoding=encoding) as f:
                        content = f.read()
                    break
                except UnicodeDecodeError:
                    continue
            
            if content:
                # BeautifulSoupìœ¼ë¡œ ì •ë¦¬ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                soup = BeautifulSoup(content, 'html.parser')
                text_content = soup.get_text()
                
                # ì²˜ìŒ 2000ìë§Œ í‘œì‹œ
                if len(text_content) > 2000:
                    result.append(f"ğŸ“ ë‚´ìš© (ì²˜ìŒ 2000ì):")
                    result.append(text_content[:2000] + "\n...")
                else:
                    result.append(f"ğŸ“ ì „ì²´ ë‚´ìš©:")
                    result.append(text_content)
                    
        elif file_ext == '.txt':
            # í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
            encodings = ['utf-8', 'cp949', 'euc-kr']
            content = None
            
            for encoding in encodings:
                try:
                    with open(target_file, 'r', encoding=encoding) as f:
                        content = f.read()
                    break
                except UnicodeDecodeError:
                    continue
            
            if content:
                if len(content) > 2000:
                    result.append(f"ğŸ“ ë‚´ìš© (ì²˜ìŒ 2000ì):")
                    result.append(content[:2000] + "\n...")
                else:
                    result.append(f"ğŸ“ ì „ì²´ ë‚´ìš©:")
                    result.append(content)
        else:
            result.append(f"âš ï¸  {file_ext} íŒŒì¼ì€ ì§ì ‘ ì½ê¸°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            result.append("ğŸ’¡ analyze_extracted_dart_document í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.")
        
        return "\n".join(result)
        
    except Exception as e:
        return f"âŒ íŒŒì¼ ì½ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def analyze_extracted_dart_document(rcept_no: str, user_query: str = "", analysis_focus: str = "all", download_folder: str = "./downloads") -> str:
    """
    ì••ì¶• í•´ì œëœ DART ê³µì‹œì„œë¥˜ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        rcept_no: ì ‘ìˆ˜ë²ˆí˜¸ (14ìë¦¬) 
        user_query: ì‚¬ìš©ì ì§ˆë¬¸ (ì„ íƒì‚¬í•­)
        analysis_focus: ë¶„ì„ ì´ˆì  ("financial", "governance", "business", "all")
        download_folder: ì••ì¶• í•´ì œëœ í´ë”ê°€ ìˆëŠ” ê²½ë¡œ (ê¸°ë³¸ê°’: ./downloads)
        
    Returns:
        ë¶„ì„ ê²°ê³¼
    """
    try:
        # ì••ì¶• í•´ì œëœ í´ë” ê²½ë¡œ
        extract_folder = os.path.join(download_folder, f"extracted_{rcept_no}")
        
        if not os.path.exists(extract_folder):
            return f"âŒ ì••ì¶• í•´ì œëœ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {extract_folder}\në¨¼ì € download_and_extract_dart_document í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        
        # ì„ì‹œ ZIP íŒŒì¼ ìƒì„± (DartZipProcessor í˜¸í™˜ì„±ì„ ìœ„í•´)
        temp_zip_path = os.path.join(download_folder, f"temp_{rcept_no}.zip")
        
        with zipfile.ZipFile(temp_zip_path, 'w', zipfile.ZIP_DEFLATED) as zip_ref:
            for root, dirs, files in os.walk(extract_folder):
                for file in files:
                    file_path = os.path.join(root, file)
                    # í´ë” êµ¬ì¡° ìœ ì§€í•˜ë©´ì„œ ZIPì— ì¶”ê°€
                    arcname = os.path.relpath(file_path, extract_folder)
                    zip_ref.write(file_path, arcname)
        
        # ZIP íŒŒì¼ ë¶„ì„
        response = zip_processor.process_document_zip(temp_zip_path, user_query, analysis_focus)
        
        # ì„ì‹œ ZIP íŒŒì¼ ì‚­ì œ
        os.remove(temp_zip_path)
        
        if response["status"] != "success":
            return f"âŒ ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨: {response.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
        
        # ì‚¬ìš©ì ì¹œí™”ì  í˜•íƒœë¡œ í¬ë§·íŒ…
        formatted_response = zip_processor.format_for_display(response)
        
        # í—¤ë” ì¶”ê°€
        result = []
        result.append(f"ğŸ“Š DART ê³µì‹œì„œë¥˜ ë¶„ì„ ê²°ê³¼ (ì ‘ìˆ˜ë²ˆí˜¸: {rcept_no})")
        result.append("=" * 60)
        result.append(formatted_response)
        
        return "\n".join(result)
        
    except Exception as e:
        return f"âŒ ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def parse_xml_file_to_readable(rcept_no: str, filename: str, download_folder: str = "./downloads", show_full_content: bool = False, max_length: int = 10000) -> str:
    """
    ì••ì¶• í•´ì œëœ XML íŒŒì¼ì„ ì‚¬ìš©ì ì¹œí™”ì ì¸ í˜•íƒœë¡œ íŒŒì‹±í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤.
    
    Args:
        rcept_no: ì ‘ìˆ˜ë²ˆí˜¸ (14ìë¦¬)
        filename: XML íŒŒì¼ëª… (í™•ì¥ì í¬í•¨)
        download_folder: ë‹¤ìš´ë¡œë“œ í´ë” ê²½ë¡œ
        show_full_content: Trueì´ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ë‚´ìš© í‘œì‹œ, Falseì´ë©´ êµ¬ì¡°í™”ëœ ì •ë³´ í‘œì‹œ
        max_length: ì „ì²´ ë‚´ìš© í‘œì‹œ ì‹œ ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´ (ê¸°ë³¸ê°’: 10000ì)
        
    Returns:
        íŒŒì‹±ëœ XML ë‚´ìš© ë˜ëŠ” ì˜¤ë¥˜ ë©”ì‹œì§€
    """
    try:
        extract_folder = os.path.join(download_folder, f"extracted_{rcept_no}")
        
        if not os.path.exists(extract_folder):
            return f"âŒ ì••ì¶• í•´ì œëœ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {extract_folder}\në¨¼ì € download_and_extract_dart_document í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        
        # XML íŒŒì¼ ì°¾ê¸°
        target_file = None
        for root, dirs, files in os.walk(extract_folder):
            for file in files:
                if (file == filename or file.lower() == filename.lower()) and file.lower().endswith('.xml'):
                    target_file = os.path.join(root, file)
                    break
            if target_file:
                break
        
        if not target_file:
            # ì‚¬ìš©ê°€ëŠ¥í•œ XML íŒŒì¼ ëª©ë¡ ì œê³µ
            xml_files = []
            for root, dirs, files in os.walk(extract_folder):
                for file in files:
                    if file.lower().endswith('.xml'):
                        xml_files.append(file)
            
            result = [f"âŒ '{filename}' XML íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
            if xml_files:
                result.append("\nğŸ“„ ì‚¬ìš© ê°€ëŠ¥í•œ XML íŒŒì¼ë“¤:")
                for xml_file in xml_files[:10]:
                    result.append(f"   â€¢ {xml_file}")
                if len(xml_files) > 10:
                    result.append(f"   ... ë° {len(xml_files) - 10}ê°œ ì¶”ê°€ XML íŒŒì¼")
            else:
                result.append("\nâš ï¸  ì••ì¶• í•´ì œëœ í´ë”ì— XML íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            return "\n".join(result)
        
        # XML íŒŒì¼ ì½ê¸° ë° íŒŒì‹±
        encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1']
        xml_content = None
        
        for encoding in encodings:
            try:
                with open(target_file, 'r', encoding=encoding) as f:
                    xml_content = f.read()
                break
            except UnicodeDecodeError:
                continue
        
        if not xml_content:
            return f"âŒ XML íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"
        
        result = []
        result.append(f"ğŸ“„ XML íŒŒì¼ íŒŒì‹± ê²°ê³¼: {filename}")
        result.append("=" * 60)
        result.append(f"ğŸ“ íŒŒì¼ ê²½ë¡œ: {target_file}")
        result.append(f"ğŸ’¾ íŒŒì¼ í¬ê¸°: {os.path.getsize(target_file) / 1024:.1f} KB")
        result.append("")
        
        # BeautifulSoupìœ¼ë¡œ XML íŒŒì‹±
        soup = BeautifulSoup(xml_content, 'xml')
        
        # show_full_contentê°€ Trueì´ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ë‚´ìš©ë§Œ í‘œì‹œ
        if show_full_content:
            result.append("ğŸ“ ë¬¸ì„œ ì „ì²´ ë‚´ìš©:")
            result.append("-" * 40)
            
            # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_text = soup.get_text()
            
            # ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
            lines = full_text.split('\n')
            cleaned_lines = []
            for line in lines:
                line = line.strip()
                if line:  # ë¹ˆ ì¤„ ì œì™¸
                    cleaned_lines.append(line)
            
            cleaned_text = '\n'.join(cleaned_lines)
            
            if len(cleaned_text) > max_length:
                result.append(f"ğŸ“‹ ì „ì²´ ë‚´ìš© (ì²˜ìŒ {max_length:,}ì, ì „ì²´ {len(cleaned_text):,}ì):")
                result.append("")
                # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìë¥´ê¸° ì‹œë„
                truncated_text = cleaned_text[:max_length]
                last_period = truncated_text.rfind('.')
                if last_period > max_length * 0.8:  # 80% ì´í›„ì— ë§ˆì¹¨í‘œê°€ ìˆìœ¼ë©´ ê±°ê¸°ì„œ ìë¥´ê¸°
                    truncated_text = truncated_text[:last_period + 1]
                
                result.append(truncated_text)
                result.append("")
                result.append(f"... (ë‚˜ë¨¸ì§€ {len(cleaned_text) - len(truncated_text):,}ì ìƒëµ)")
                result.append("\nğŸ’¡ ì „ì²´ ë‚´ìš©ì„ ë³´ë ¤ë©´ max_length íŒŒë¼ë¯¸í„°ë¥¼ ëŠ˜ë ¤ì£¼ì„¸ìš”.")
            else:
                result.append(f"ğŸ“‹ ì „ì²´ ë‚´ìš© ({len(cleaned_text):,}ì):")
                result.append("")
                result.append(cleaned_text)
            
            return "\n".join(result)
        
        # ê¸°ë³¸ ëª¨ë“œ: êµ¬ì¡°í™”ëœ ì •ë³´ í‘œì‹œ
        result.extend(_extract_structured_xml_info(soup))
        
        return "\n".join(result)
        
    except Exception as e:
        return f"âŒ XML íŒŒì¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def _extract_structured_xml_info(soup: BeautifulSoup) -> list:
    """XMLì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ"""
    result = []
    
    # 1. ë¬¸ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
    result.append("ğŸ“‹ ë¬¸ì„œ ê¸°ë³¸ ì •ë³´:")
    result.append("-" * 30)
    
    # ì¼ë°˜ì ì¸ DART XML í•„ë“œë“¤ ì°¾ê¸°
    basic_fields = {
        'corp_name': 'íšŒì‚¬ëª…',
        'corp_code': 'íšŒì‚¬ì½”ë“œ', 
        'report_nm': 'ë³´ê³ ì„œëª…',
        'rcept_dt': 'ì ‘ìˆ˜ì¼ì',
        'flr_nm': 'ì œì¶œì¸',
        'bsns_year': 'ì‚¬ì—…ì—°ë„',
        'reprt_code': 'ë³´ê³ ì„œì½”ë“œ'
    }
    
    for field, label in basic_fields.items():
        element = soup.find(field)
        if element and element.get_text().strip():
            result.append(f"{label}: {element.get_text().strip()}")
    
    # 2. ë¬¸ì„œ ë‚´ìš© ì„¹ì…˜ ì¶”ì¶œ
    result.append(f"\nğŸ“„ ë¬¸ì„œ ì£¼ìš” ë‚´ìš©:")
    result.append("-" * 30)
    
    # ì£¼ìš” í…ìŠ¤íŠ¸ ì„¹ì…˜ ì°¾ê¸° (p, div, section ë“±)
    content_sections = []
    
    # ì¼ë°˜ì ì¸ ì½˜í…ì¸  íƒœê·¸ë“¤ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    content_tags = soup.find_all(['p', 'div', 'section', 'article', 'span'])
    for tag in content_tags:
        text = tag.get_text().strip()
        # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ ì„ ë³„ (50ì ì´ìƒ, 3000ì ì´í•˜)
        if 50 <= len(text) <= 3000:
            # ì¤‘ë³µ ì œê±° (ì´ë¯¸ í¬í•¨ëœ í…ìŠ¤íŠ¸ëŠ” ì œì™¸)
            is_duplicate = False
            for existing in content_sections:
                if text in existing or existing in text:
                    is_duplicate = True
                    break
            if not is_duplicate:
                content_sections.append(text)
    
    # ì½˜í…ì¸  ì„¹ì…˜ í‘œì‹œ (ìµœëŒ€ 5ê°œ)
    if content_sections:
        result.append("ğŸ“ ì£¼ìš” ë¬¸ì„œ ë‚´ìš©:")
        for i, content in enumerate(content_sections[:5]):
            result.append(f"\nğŸ”¸ ì„¹ì…˜ {i+1}:")
            # ê¸´ í…ìŠ¤íŠ¸ëŠ” ì¤„ë°”ê¿ˆìœ¼ë¡œ ì •ë¦¬
            formatted_content = content.replace('. ', '.\n   ')
            if len(formatted_content) > 1000:
                result.append(f"   {formatted_content[:1000]}...")
            else:
                result.append(f"   {formatted_content}")
    
    # 3. í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ
    tables = soup.find_all(['table', 'list'])
    if tables:
        result.append(f"\nğŸ“Š ë°ì´í„° í…Œì´ë¸” ({len(tables)}ê°œ ë°œê²¬):")
        result.append("-" * 30)
        
        for i, table in enumerate(tables[:3]):  # ì²˜ìŒ 3ê°œ í…Œì´ë¸”ë§Œ
            result.extend(_extract_table_data(table, i))
    
    # 4. íŠ¹ì • í‚¤ì›Œë“œ ê¸°ë°˜ ì •ë³´ ì¶”ì¶œ
    keyword_sections = {
        'ğŸ’° ì¬ë¬´ì •ë³´': ['ìì‚°', 'ë¶€ì±„', 'ìë³¸', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'ìˆœì´ìµ', 'ë°°ë‹¹', 'ì£¼ê°€'],
        'ğŸ‘¥ íšŒì‚¬ì •ë³´': ['ëŒ€í‘œì´ì‚¬', 'ë³¸ì ', 'ì„¤ë¦½', 'ì§ì›', 'ì‚¬ì—…ëª©ì ', 'ì£¼ìš”ì‚¬ì—…'],
        'ğŸ“ˆ ì‹¤ì ì •ë³´': ['ë§¤ì¶œì•¡', 'ì˜ì—…ì‹¤ì ', 'ì‹œì¥ì ìœ ìœ¨', 'ì„±ì¥ë¥ ', 'ìˆ˜ìµë¥ '],
        'ğŸ›ï¸ ì§€ë°°êµ¬ì¡°': ['ì£¼ì£¼', 'ì´ì‚¬íšŒ', 'ê°ì‚¬', 'ì„ì›', 'ì§€ë¶„ìœ¨']
    }
    
    for section_name, keywords in keyword_sections.items():
        found_info = []
        for keyword in keywords:
            # í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ìš”ì†Œë“¤ ì°¾ê¸°
            elements = soup.find_all(text=re.compile(keyword, re.IGNORECASE))
            for elem in elements[:2]:  # ê° í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 2ê°œ
                parent = elem.parent
                if parent:
                    text = parent.get_text().strip()
                    # ì ì ˆí•œ ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ë§Œ ì„ ë³„
                    if 20 <= len(text) <= 500 and keyword.lower() in text.lower():
                        # ì¤‘ë³µ ì œê±°
                        if not any(existing in text or text in existing for existing in found_info):
                            found_info.append(text)
        
        if found_info:
            result.append(f"\n{section_name}:")
            result.append("-" * 25)
            for info in found_info[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                result.append(f"â€¢ {info}")
    
    # 5. XML êµ¬ì¡° ìš”ì•½
    result.append(f"\nğŸ”§ ë¬¸ì„œ êµ¬ì¡° ì •ë³´:")
    result.append("-" * 30)
    
    # ìµœìƒìœ„ íƒœê·¸ë“¤ ì°¾ê¸°
    if soup.find():
        root_tag = soup.find()
        result.append(f"ë¬¸ì„œ ìœ í˜•: <{root_tag.name}>")
        
        # ì£¼ìš” í•˜ìœ„ íƒœê·¸ë“¤ (ì§ì ‘ ìì‹ë§Œ)
        child_tags = set()
        for child in root_tag.find_all(recursive=False):
            if child.name:
                child_tags.add(child.name)
        
        if child_tags:
            result.append(f"ì£¼ìš” ì„¹ì…˜: {', '.join(sorted(child_tags)[:10])}")
    
    # ì „ì²´ ìš”ì†Œ ìˆ˜ì™€ í…ìŠ¤íŠ¸ ê¸¸ì´
    all_elements = soup.find_all()
    total_text = soup.get_text()
    result.append(f"XML ìš”ì†Œ ìˆ˜: {len(all_elements)}ê°œ")
    result.append(f"ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(total_text):,}ì")
    
    result.append(f"\nğŸ’¡ ë” ìƒì„¸í•œ ë¶„ì„ì„ ì›í•˜ì‹œë©´ analyze_extracted_dart_document í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    return result


def _extract_table_data(table, index: int) -> list:
    """í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ"""
    result = []
    
    if table.name == 'table':
        result.append(f"\nğŸ“ˆ í…Œì´ë¸” {index+1}:")
        rows = table.find_all('tr')
        if rows:
            # í…Œì´ë¸” í—¤ë” (ì²« ë²ˆì§¸ í–‰)
            if rows:
                header_row = rows[0]
                header_cells = header_row.find_all(['td', 'th'])
                if header_cells:
                    headers = [cell.get_text().strip() for cell in header_cells]
                    result.append(f"   ğŸ“‹ ì»¬ëŸ¼: {' | '.join(headers)}")
            
            # ë°ì´í„° í–‰ë“¤ (ìµœëŒ€ 10í–‰)
            for row_idx, row in enumerate(rows[1:11]):  # í—¤ë” ì œì™¸í•˜ê³  ìµœëŒ€ 10í–‰
                cells = row.find_all(['td', 'th'])
                if cells:
                    row_data = [cell.get_text().strip() for cell in cells]
                    # ë¹ˆ í–‰ ì œì™¸
                    if any(data.strip() for data in row_data):
                        result.append(f"   {row_idx+1:2d}. {' | '.join(row_data)}")
            
            if len(rows) > 11:
                result.append(f"   ... ë° {len(rows) - 11}ê°œ ì¶”ê°€ í–‰")
    
    elif table.name == 'list':
        result.append(f"\nğŸ“‹ ë¦¬ìŠ¤íŠ¸ {index+1}:")
        items = table.find_all(recursive=False)
        for idx, item in enumerate(items[:10]):  # ìµœëŒ€ 10ê°œ í•­ëª©
            item_text = item.get_text().strip()
            if item_text:
                if len(item_text) > 200:
                    result.append(f"   {idx+1}. {item_text[:200]}...")
                else:
                    result.append(f"   {idx+1}. {item_text}")
    
    return result